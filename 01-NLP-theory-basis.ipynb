{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "English tokenize\n",
    "\"\"\"\n",
    "import nltk\n",
    "\n",
    "sentence = 'hello, world'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.010 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 福建/ 福州/ 福州大学/ 大学\n",
      "Default Mode: 我/ 来到/ 福建/ 福州大学\n",
      "他, 来到, 了, 望京, 大厦\n",
      "刘涛, 硕士, 毕业, 于, 厦门, 大学, 厦门大学, ，, 后, 在, 日本, 东京, 大学, 日本东京大学, 深造\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chinese tokenize\n",
    "\"\"\"\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到福建福州大学\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/ \".join(seg_list))  # 全模式\n",
    "seg_list = jieba.cut(\"我来到福建福州大学\", cut_all=False)\n",
    "print(\"Default Mode:\", \"/ \".join(seg_list)) # 精确模式\n",
    "seg_list = jieba.cut(\"他来到了望京大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "seg_list = jieba.cut_for_search(\"刘涛硕士毕业于厦门大学，后在日本东京大学深造\")\n",
    "# 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n",
      "['RT', '@angelababy', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Social language tokenize\n",
    "\"\"\"\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))\n",
    "\n",
    "emotions_str = r\"\"\"\n",
    "(?:\n",
    "    [:=;]  # eyes\n",
    "    [oO\\-]?  # nose\n",
    "    [D\\)\\]\\(\\]/\\\\OpP]  # mouth\n",
    ")\"\"\"\n",
    "regex_str = [\n",
    "    emotions_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @somebody\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # topic tag\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # number\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and‘\n",
    "    r'(?:[\\w_]+)', # others\n",
    "    r'(?:\\S)' # others\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emotion_re = re.compile(r'^'+emotions_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokens_re.findall(s)\n",
    "    if lowercase:\n",
    "        # emotion can not be lower\n",
    "        tokens = [token if emotion_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parts of speech\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
