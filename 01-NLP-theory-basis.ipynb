{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "English tokenize\n",
    "\"\"\"\n",
    "import nltk\n",
    "\n",
    "\n",
    "sentence = 'hello, world'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.200 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 福建/ 福州/ 福州大学/ 大学\n",
      "Default Mode: 我/ 来到/ 福建/ 福州大学\n",
      "他, 来到, 了, 望京, 大厦\n",
      "刘涛, 硕士, 毕业, 于, 厦门, 大学, 厦门大学, ，, 后, 在, 日本, 东京, 大学, 日本东京大学, 深造\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chinese tokenize\n",
    "\"\"\"\n",
    "import jieba\n",
    "\n",
    "\n",
    "seg_list = jieba.cut(\"我来到福建福州大学\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/ \".join(seg_list))  # 全模式\n",
    "seg_list = jieba.cut(\"我来到福建福州大学\", cut_all=False)\n",
    "print(\"Default Mode:\", \"/ \".join(seg_list)) # 精确模式\n",
    "seg_list = jieba.cut(\"他来到了望京大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "seg_list = jieba.cut_for_search(\"刘涛硕士毕业于厦门大学，后在日本东京大学深造\")\n",
    "# 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n",
      "['RT', '@angelababy', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Social language tokenize\n",
    "\"\"\"\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))\n",
    "\n",
    "emotions_str = r\"\"\"\n",
    "(?:\n",
    "    [:=;]  # eyes\n",
    "    [oO\\-]?  # nose\n",
    "    [D\\)\\]\\(\\]/\\\\OpP]  # mouth\n",
    ")\"\"\"\n",
    "regex_str = [\n",
    "    emotions_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @somebody\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # topic tag\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # number\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and‘\n",
    "    r'(?:[\\w_]+)', # others\n",
    "    r'(?:\\S)' # others\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emotion_re = re.compile(r'^'+emotions_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokens_re.findall(s)\n",
    "    if lowercase:\n",
    "        # emotion can not be lower\n",
    "        tokens = [token if emotion_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- PorterStemmer -------\n",
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0a520e6d25c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporter_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multiply'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporter_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'provision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'went'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wenting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parts of speech\n",
    "词性归一\n",
    "Stemming 词干提取 ---> 直接砍尾巴 \n",
    "Lemmatization 词形归⼀ ---> 各种类型词性归为一个形式\n",
    "\"\"\"\n",
    "print(\"------- PorterStemmer -------\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('maximum'))\n",
    "print(porter_stemmer.stem('presumably'))\n",
    "print(porter_stemmer.stem('multiply'))\n",
    "print(porter_stemmer.stem('provision'))\n",
    "print(porter_stemmer.stem('went'))\n",
    "print(porter_stemmer.stem('wenting'))\n",
    "\n",
    "print(\"------- SnowballStemmer -------\")\n",
    "from nltk.stem import SnowballStemmer\n",
    "# 偷懒用Snowball\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print(snowball_stemmer.stem('maximum'))\n",
    "print(snowball_stemmer.stem('presumably'))\n",
    "\n",
    "print(\"------- LancasterStemmer -------\")\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print(lancaster_stemmer.stem('maximum'))\n",
    "print(lancaster_stemmer.stem('presumably'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Lema\n",
    "语言学家弄出的英语语料网络\n",
    "坏处：永远需要更新，新词无法detail出来\n",
    "好处：预处理会得到便利，复数单一化\n",
    "\"\"\"\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('dogs'))\n",
    "print(wordnet_lemmatizer.lemmatize('churches'))\n",
    "print(wordnet_lemmatizer.lemmatize('aardwolves'))\n",
    "print(wordnet_lemmatizer.lemmatize('abaci'))\n",
    "print(wordnet_lemmatizer.lemmatize('hardrock'))\n",
    "\n",
    "# Lema：how to recognize Went？ v.go or n.Went\n",
    "print(wordnet_lemmatizer.lemmatize('are'))\n",
    "print(wordnet_lemmatizer.lemmatize('is'))\n",
    "print(wordnet_lemmatizer.lemmatize('are', pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize('is', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NLTK POS Tag\n",
    "This is import in English\n",
    "\"\"\"\n",
    "import nltk\n",
    "text = nltk.word_tokenize('This quiet chant shall relieve your wasted heart')\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stopwords\n",
    "对于注重理解文本“意思”的应用场景来说歧义太多\n",
    "英文停止词列表：https://www.ranks.nl/stopwords\n",
    "\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "# tokenize get a word list\n",
    "word_list = nltk.word_tokenize('you are the apple of my eyes')\n",
    "# filter\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sentiment analysis\n",
    "最简单 sentiment dictionary\n",
    "like 1\n",
    "good 2\n",
    "bad -2\n",
    "terrible -3\n",
    "类似于关键词打分机制\n",
    "http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010\n",
    "\"\"\"\n",
    "words = \"What a terrible trip\".split(' ')\n",
    "sentiment_dictionary = {}\n",
    "for line in open('AFINN/AFINN-111.txt', 'r'):\n",
    "    word, score = line.split('\\t')\n",
    "    sentiment_dictionary[word] = int(score)\n",
    "total_score = sum(sentiment_dictionary.get(word, 0) for word in words)\n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sentiment analysis with Machine Learning\n",
    "\"\"\"\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "# 随⼿手造点训练集\n",
    "s1 = 'this is a good book'\n",
    "s2 = 'this is a awesome book'\n",
    "s3 = 'this is a bad book'\n",
    "s4 = 'this is a terrible book'\n",
    "def preprocess(s):\n",
    "    \"\"\"\n",
    "    :param s: str\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "# Func: 句子处理\n",
    "# 这⾥简单的用了split(),把句子中每个单词分开\n",
    "# 显然还有更多的processing method可以用\n",
    "    return {word: True for word in s.lower().split()}\n",
    "# return: {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}\n",
    "# 这里我们用最简单的True,来表示,这个词『出现在当前的句子中』的意义。\n",
    "training_data = [[preprocess(s1), 'pos'],\n",
    "                 [preprocess(s2), 'pos'],\n",
    "                 [preprocess(s3), 'neg'],\n",
    "                 [preprocess(s4), 'neg']]\n",
    "model = NaiveBayesClassifier.train(training_data)\n",
    "print(model.classify(preprocess('this is a good book')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'my', 'sentence', 'this', 'is', 'my', 'life', 'this', 'is', 'the', 'day']\n",
      "3\n",
      "[('this', 3), ('is', 3), ('my', 2), ('sentence', 1), ('life', 1), ('the', 1), ('day', 1)]\n",
      "{'this': 0, 'is': 1, 'my': 2, 'sentence': 3, 'life': 4, 'the': 5, 'day': 6}\n",
      "[1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Application: Text Similarity\n",
    "Frequency 频率统计\n",
    "\"\"\"\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "corpus = 'this is my sentence ' \\\n",
    "         'this is my life ' \\\n",
    "         'this is the day'\n",
    "\n",
    "token = nltk.word_tokenize(corpus)\n",
    "print(token)\n",
    "# us FreqDist to count the word's frequency\n",
    "fdist = FreqDist(token)\n",
    "print(fdist['is'])\n",
    "\n",
    "# 拿出最常用的50个单词\n",
    "standard_freq_vector = fdist.most_common(50)\n",
    "size = len(standard_freq_vector)\n",
    "print(standard_freq_vector)\n",
    "\n",
    "def position_lookup(v):\n",
    "    res = {word[0]: k for k, word in enumerate(v)}\n",
    "    return res\n",
    "\n",
    "standard_position_dict = position_lookup(standard_freq_vector)\n",
    "\n",
    "print(standard_position_dict)\n",
    "\n",
    "# 新句子\n",
    "sentence = \"this is cool\"\n",
    "# 建立一个和我们标准vector同样大小的向量\n",
    "freq_vector = [0] * size\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "for word in tokens:\n",
    "    try:\n",
    "        # 如果在我们的词库⾥里里出现过\n",
    "        # 那么就在\"标准位置\"上+1\n",
    "        freq_vector[standard_position_dict[word]] += 1\n",
    "    except KeyError:\n",
    "        # 如果是个新词\n",
    "        # 就pass掉\n",
    "        continue\n",
    "print(freq_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Application: Text Categorization\n",
    "TF-IDF\n",
    "Term Frequency, 衡量⼀个term在⽂档中出现得有多频繁。\n",
    "IDF: Inverse Document Frequency, 衡量⼀个term有多重要。\n",
    "TF-IDF = TF * IDF\n",
    "\"\"\"\n",
    "from nltk.text import TextCollection\n",
    "# 首先,把所有的文档放到TextCollection类中。\n",
    "# 这个类会自动帮你断句,做统计,做计算\n",
    "corpus = TextCollection(['this is sentence one',\n",
    "                         'this is sentence two',\n",
    "                         'this is sentence three'])\n",
    "# 直接就能算出tfidf\n",
    "# (term:一句话中的某个term, text:这句话)\n",
    "print(corpus.tf_idf('this', 'this is sentence four'))\n",
    "# 0.0\n",
    "# 同理,怎么得到⼀一个标准大小的vector来表示所有的句子?\n",
    "# 对于每个新句子\n",
    "new_sentence = 'this is sentence five'\n",
    "# 遍历一遍所有的vocabulary中的词:\n",
    "for word in token:\n",
    "    print(corpus.tf_idf(word, new_sentence))\n",
    "# 我们会得到⼀一个巨⻓长(=所有vocab⻓长度)的向量量\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
